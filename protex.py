from prettytable import PrettyTable
import pandas as pd 
import numpy as np
import torch

import sys
sys.path.append('/home/prichter/Documents/protex/src')
from src.dataset import SequenceDataset
from src.esm import EsmClassifierV1, EsmClassifierV2, esm_test, esm_train
from src.bench import BenchmarkClassifier, BenchmarkTokenizer
from torch.utils.data import DataLoader
from transformers import EsmModel, EsmTokenizer

from src.plot import *

figure_dir = '/home/prichter/Documents/protex/figures/'
data_dir = '/home/prichter/Documents/protex/data/'

def generate_esm_embedding(sequences):
    '''
    Tokenize an input amino acid sequence, and plug it into the ESM model. Produces
    an embedding of the sequence in a new latent space. 

    args:
        - sequences (list): A list of amino acid sequences, given as strings. 
    '''
    model = EsmModel.from_pretrained('facebook/esm2_t6_8M_UR50D')
    inputs = EsmTokenizer(sequences)
    

# def summary(model):
#     '''
#     Print out a summary of the model weights, and which parameters are trainable. 
#     '''
#     table = PrettyTable(['name', 'num_params', 'fixed'])

#     num_fixed = 0
#     num_total = 0

#     params = {}

#     for name, param in model.named_parameters():
#         num_params = param.numel()
#         fixed = str(not param.requires_grad)
#         table.add_row([name, num_params, fixed])

#         if not param.requires_grad:
#             num_fixed += num_params
        
#         num_total += num_params
    
#     print(table)
#     print('TOTAL:', num_total)
#     print('TRAINABLE:', num_total - num_fixed, f'({int(100 * (num_total - num_fixed)/num_total)}%)')


if __name__ == '__main__':

    # ------------------------------------------------------------------------------------------
    # Because of the poor performance of V2 of the ESM classifier, I want to confirm that embeddings
    # are correctly matched up with the original data. 
    # ------------------------------------------------------------------------------------------


    # ------------------------------------------------------------------------------------------
    # One of the first things I want to do is see how the ESM-generated embeddings and the
    # ones generated by the BenchmarkTokenizer compare in lower-dimensional space. Can we see,
    # from the embeddings alone, differences in selenoproteins verus short proteins?

    # Should probably note that, if we can see a difference with the BenchmarkTokenizer alone, 
    # the ML models may just be picking up on differences in amino acid content. 
    # ------------------------------------------------------------------------------------------

    # embeddings_esm = pd.read_csv(data_dir + 'test_embeddings_esm.csv')
    # embeddings_bench = pd.read_csv(data_dir + 'test_embeddings_bench.csv')

    # # Get the category, i.e. selenocysteine-containing (1) or not (0)
    # categories = pd.read_csv(data_dir + 'test.csv')['label'].values
    # labels = ['selenoprotein' if (x == 1) else 'control' for x in categories]
    
    # kwargs = {'labels':labels, 'n_points':300}
    # plot_embeddings(embeddings_esm, title='ESM embeding space', n_points=300, filename=figure_dir + 'fig_03.png', **kwargs)
    # plot_embeddings(embeddings_bench, n_points=300, title='AAC embedding space', filename=figure_dir + 'fig_04.png', **kwargs)
    
    # ------------------------------------------------------------------------------------------
    # Another thing I wanted to check on is how the predictions made by different models map to
    # different embedding spaces. So, I am going to train each of the current model variations
    # (currently an embedding layer + linear classifier, simple logistic regression, and esm + linear
    # classifier), and plot where they give the correct predictions in each embedding space (i.e. the
    # one generated by ESM and the one from the BenchmarkTokenizer). Now that I am thinking about it, 
    # I should probably do something for the embedding layer of the LinearClassifier as well. 
    # ------------------------------------------------------------------------------------------

    # BenchmarkClassifier ----------------------------------------------------------------------
    
    # embeddings_esm = pd.read_csv(data_dir + 'test_embeddings_esm.csv')
    # embeddings_bench = pd.read_csv(data_dir + 'test_embeddings_bench.csv')
 
    # # Load the DataLoader. Use the test data for the time being.  
    # kwargs = {'tokenizer':BenchmarkTokenizer()}
    # data = SequenceDataset(pd.read_csv('/home/prichter/Documents/protex/data/train.csv'), **kwargs)

    # # NOTE: Should I be using the train data for the fitting step, for a valid comparison?
    # model = BenchmarkClassifier()
    # bench_model.fit(test_data.get_data(), test_data.get_labels()) 
    # preds, _ = bench_model.predict(test_data.get_data())

    # print('BenchmarkClassifier ACCURACY:', (preds == test_data.get_labels()).mean().item())

    # # Label the data according to whether or not it was correctly or incorrectly classified. 
    # labels = ['correct' if x else 'incorrect' for x in (preds == data.get_labels)]
    
    # palette = {'correct':'green', 'incorrect':'red'}
    # kwargs = {'labels':labels, 'palette':palette, 'n_points':300}
    # plot_embeddings(embeddings_bench, title='BenchmarkClassifier predictions in AAC embedding space', filename=figure_dir + 'fig_01.png', **kwargs)
    # plot_embeddings(embeddings_esm, title='BenchmarkClassifier predictions in ESM embedding space', filename=figure_dir + 'fig_02.png', **kwargs)
    
    # ------------------------------------------------------------------------------------------
    
    # EsmClassifierV2 --------------------------------------------------------------------------
    
    train_embeddings_esm = pd.read_csv(data_dir + 'train_embeddings_esm.csv')
    test_embeddings_esm = pd.read_csv(data_dir + 'test_embeddings_esm.csv')
 
    # Load the DataLoader. Use the test data for the time being.  
    kwargs = {'tokenizer':BenchmarkTokenizer()}
    data = SequenceDataset(pd.read_csv('/home/prichter/Documents/protex/data/train.csv'), **kwargs)

    # NOTE: Should I be using the train data for the fitting step, for a valid comparison?
    model = BenchmarkClassifier()
    bench_model.fit(test_data.get_data(), test_data.get_labels()) 
    preds, _ = bench_model.predict(test_data.get_data())

    print('BenchmarkClassifier ACCURACY:', (preds == test_data.get_labels()).mean().item())

    # Label the data according to whether or not it was correctly or incorrectly classified. 
    labels = ['correct' if x else 'incorrect' for x in (preds == data.get_labels)]
    
    palette = {'correct':'green', 'incorrect':'red'}
    kwargs = {'labels':labels, 'palette':palette, 'n_points':300}
    plot_embeddings(embeddings_bench, title='BenchmarkClassifier predictions in AAC embedding space', filename=figure_dir + 'fig_01.png', **kwargs)
    plot_embeddings(embeddings_esm, title='BenchmarkClassifier predictions in ESM embedding space', filename=figure_dir + 'fig_02.png', **kwargs)
    
 
    # Now use ESM V2 to generate the same plots. Load up the pre-trained model
    # esm_v2_model = EsmClassifierV2()
    test_loader = DataLoader(test_data)
    train_loader = DataLoader(train_data, batch_size=128)

    # losses = esm_train(esm_v2_model, train_loader)
    # torch.save(esm_v2_model, 'esm_v2_model.pickle') # Save the trained model. 

    esm_v2_model = torch.load('./esm_v2_model.pickle')
    output = esm_test(esm_v2_model, train_loader)

    # print('LOSS:', output['loss'])
    print('ACCURACY:', (output['preds'] == output['labels']).float().mean().item())

    # Label the data according to whether or not it was correctly or incorrectly classified. 
    correct = (output['preds'].numpy() == output['labels'].numpy())
    labels = ['EsmClassifier correct' if x else 'EsmClassifier incorrect' for x in correct]
    labels = np.array(labels)
     
    palette = {'EsmClassifier correct':'green', 'EsmClassifier incorrect':'red'}
    plot_embeddings(embeddings_bench, labels=labels, title='Content-based embeddings', n_points=300, filename=figure_dir + 'embeddings_bench_prediction_esm.png', palette=palette)
    plot_embeddings(embeddings_esm_test, labels=labels, title='ESM embeddings', n_points=300, filename=figure_dir + 'embeddings_esm_prediction_esm.png', palette=palette)

 