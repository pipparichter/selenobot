from prettytable import PrettyTable
import pandas as pd 
import numpy as np
import torch

from src.dataset import SequenceDataset
from src.esm import EsmClassifierV1, EsmClassifierV2
from src.bench import BenchmarkClassifier

from src.plot import *

figure_dir = '/home/prichter/Documents/protex/figures/'
data_dir = '/home/prichter/Documents/protex/data/'

# def summary(model):
#     '''
#     Print out a summary of the model weights, and which parameters are trainable. 
#     '''
#     table = PrettyTable(['name', 'num_params', 'fixed'])

#     num_fixed = 0
#     num_total = 0

#     params = {}

#     for name, param in model.named_parameters():
#         num_params = param.numel()
#         fixed = str(not param.requires_grad)
#         table.add_row([name, num_params, fixed])

#         if not param.requires_grad:
#             num_fixed += num_params
        
#         num_total += num_params
    
#     print(table)
#     print('TOTAL:', num_total)
#     print('TRAINABLE:', num_total - num_fixed, f'({int(100 * (num_total - num_fixed)/num_total)}%)')


if __name__ == '__main__':

    # ------------------------------------------------------------------------------------------
    # One of the first things I want to do is see how the ESM-generated embeddings and the
    # ones generated by the BenchmarkTokenizer compare in lower-dimensional space. Can we see,
    # from the embeddings alone, differences in selenoproteins verus short proteins?

    # Should probably note that, if we can see a difference with the BenchmarkTokenizer alone, 
    # the ML models may just be picking up on differences in amino acid content. 
    # ------------------------------------------------------------------------------------------

    embeddings_esm = pd.read_cav
    # tokenizer = EsmTokenizer.from_pretrained('facebook/esm2_t6_8M_UR50D')
    # kwargs = {'padding':True, 'truncation':True, 'return_tensors':'pt'}
    tokenizer = BenchmarkTokenizer

    # Grab the pre-loaded embeddings. 
    train_embeddings = pd.read_csv('/home/prichter/Documents/protex/data/train_embeddings.csv')

    train_data = SequenceDataset(pd.read_csv('/home/prichter/Documents/protex/data/train.csv'), tokenizer=tokenizer, embeddings=train_embeddings, **kwargs)
    train_loader = DataLoader(train_data, batch_size=64) # Reasonable batch size?

    model = BenchmarkClassifier()
    
    model.fit(train_loader.get_data(), train_loader.get_labels())
    preds, _ = model.predict(train_loader.get_data())

    # Label the data according to whether or not it was correctly or incorrectly classified. 
    correct = (preds == train_loader.get_labels())
    labels = np.empty(len(correct))
    labels[correct] = 'correct'
    labels[np.invert(correct)] = 'incorrect'

    plot_embeddings

    # # test_data = SequenceDataset(pd.read_csv('/home/prichter/Documents/protex/data/test.csv'), tokenizer=tokenizer, **kwargs)
    # # test_loader = DataLoader(test_data, batch_size=64) # Reasonable batch size?
    
    # model = ESMClassifierV2()
    # loss = esm_train(model, train_loader, n_epochs=200)
    # print(loss)
    
    # torch.save(model, '/home/prichter/Documents/protex/model_esm_v2.pickle')


# if __name__ == '__main__':
#     # Probably should organize the embeddings and get rid of the duplicates. 
#     encodings = pd.read_csv('./data/test_embeddings.csv', header=None).values
#     indices = pd.read_csv('./data/test_indices.csv', header=None).values

#     data = pd.DataFrame(encodings, columns=[str(i) for i in range(encodings.shape[1])])
#     data['index'] = indices
#     data = data.drop_duplicates('index')

#     data.to_csv('./data/test_embeddings_01.csv', index=False)
