{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8b68634",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "\n",
    "**NOTE:** The steps described in this notebook are not necessary for replicating Selenobot results. Pre-trained models, as well as completed training, testing, and validation datasets are available in a Google Cloud bucket, and instructions for downloading them can be found in the `testing.ipynb` and `training.ipynb` notebooks.\n",
    "\n",
    "If you want to run this code, be sure to modify the `DATA_DIR` variable below to specify where the data will be stored on your machine. `DATA_DIR` is the absolute path specifying the location where the data will be stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089b964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/prichter/Documents/data/testing/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f469e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the selenobot/ subirectory to the module search path, so that the modules in this directory are visible from the notebook.\n",
    "sys.path.append('../selenobot/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2c2519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import Dataset, get_dataloader \n",
    "from embedders import AacEmbedder, LengthEmbedder\n",
    "from classifiers import Classifier, SimpleClassifier\n",
    "import pandas as pd\n",
    "from typing import NoReturn\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e470853",
   "metadata": {},
   "source": [
    "## Downloading training and validation data\n",
    "\n",
    "Training and validation data used in this project is available for download from a Google Cloud bucket, and can be accessed using the URLs below. These datasets can also be generated from scratch by following the procedure in the `setup.ipynb` notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6974d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the training data from Google Cloud.\n",
    "! curl https://storage.googleapis.com/selenobot-data/train.csv -o '{DATA_DIR}train.csv'\n",
    "# Download the validation data from Google Cloud.\n",
    "! curl https://storage.googleapis.com/selenobot-data/val.csv -o '{DATA_DIR}val.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bbb67",
   "metadata": {},
   "source": [
    "## Instantiating `Dataset`s\n",
    "\n",
    "Datasets are Pytorch objects which store data and provide a consistent interface for accessing that data. For the Selenobot project, we define custom Dataset objects which provide extra functionality for embedding sequence data. The custom `Dataset`s take a pandas `DataFrame` and an `Embedder` object as input (either a `LengthEmbedder` or `AacEmbedder`). The embedder will be applied to the sequences in the `DataFrame`, and the resulting vectors stored in the `embeddings` attribute of the `Dataset`. If no embedder is specified, it is assumed that the embeddings are already contained in the input DataFrame, and are extracted into the embeddings attribute (this is necessary when using PLM embeddings, as they cannot be generated *ad hoc* due to computational cost). \n",
    "\n",
    "For training, three `Datasets` are instantiated using the training data. Each `Dataset` uses a different embedder: one to train the AAC classifier, one to train the length classifier, and one to train the Selenobot (PLM embedding-based) classifier. For each of these `Dataset`s, a corresponding `Dataset` (with the same embedder) is created to store the validation data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8303b956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the validation and training data into pandas DataFrames.\n",
    "train_df = pd.read_csv(f'{DATA_DIR}train.csv')\n",
    "val_df = pd.read_csv(f'{DATA_DIR}val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57d7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_train_dataset = Dataset(train_df, embedder=LengthEmbedder())\n",
    "len_val_dataset = Dataset(val_df, embedder=LengthEmbedder())\n",
    "\n",
    "aac_train_dataset = Dataset(train_df, embedder=AacEmbedder())\n",
    "aac_val_dataset = Dataset(val_df, embedder=AacEmbedder())\n",
    "\n",
    "sel_train_dataset = Dataset(train_df, embedder=None)\n",
    "sel_val_dataset = Dataset(val_df, embedder=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fac4e3",
   "metadata": {},
   "source": [
    "## Instantiating `DataLoader`s\n",
    "\n",
    "Model training is mediated by Pytorch `DataLoaders`, which are objects that facilitate batch training. In order to address the imbalance in the training data (many more full-length proteins than truncated selenoproteins), we implemented a custom Pytorch `BatchSampler`, called  `BalancedBatchSampler`, which is defined in the `dataset.py` file. `BalancedBatchSampler` is used in conjunction with the `DataLoaders` to ensure that each batch has an equal number of truncated selenoproteins and non-selenoproteins. It does so by repeatedly resampling from the training data, using the algorithm described in the `dataset.py` file. \n",
    "\n",
    "`DataLoaders` are created using the `get_dataloader` function, which takes a `Dataset` as input, and returns a balanced-batch `DataLoader` object with the specified batch size. For all models in this investigation, we used batches of size 1024. No other batch sizes were tested. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd4a089",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the batch size.\n",
    "batch_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006416c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate DataLoaders for each training Dataset.\n",
    "len_dataloader = get_dataloader(aac_train_dataset, batch_size=batch_size)\n",
    "aac_dataloader = get_dataloader(aac_train_dataset, batch_size=batch_size)\n",
    "sel_dataloader = get_dataloader(sel_train_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d294ec1",
   "metadata": {},
   "source": [
    "## Training the models\n",
    "\n",
    " Two types of classifiers are used in this investigation: the `Classifier` (for AAC and PLM embeddings) and the `SimpleClassifier` (for length embeddings). Each of these models is defined in the `classifiers.py` file. Both classes implement a fit method, which takes a `DataLoader` and validation `Dataset` as input, as well as other parameters such as learning rate and epochs. This method trains the model using the `DataLoader`, and returns a `Reporter` object, which stores information regarding model performance during the training process.\n",
    "\n",
    "Each model is trained for `10` epochs, which was found to be sufficient for convergence. The learning rate was also held constant at `0.001`. Upon completion, the weights of the trained models are stored as PTH files using the PyTorch `save` method. The Reporter objects are also stored using the Python `pickle` module. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284fa27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate each model with the appropriate dimensions.\n",
    "len_model = SimpleClassifier(latent_dim=1)\n",
    "aac_model = Classifier(latent_dim=21, hidden_dim=8)\n",
    "sel_model = Classifier(latent_dim=1024, hidden_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d0ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store variables for the learning rate and number of epochs.\n",
    "lr = 0.001\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bc4687",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_reporter = len_model.fit(len_dataloader, val_dataset=len_val_dataset, embedder=embedder), epochs=epochs, lr=lr)\n",
    "# Save the Reporter object using pickle.\n",
    "with open(f'{DATA_DIR}len_reporter.pkl', 'wb') as f:\n",
    "        pickle.dump(train_reporter, f)\n",
    "# Save the weights of the trained model. \n",
    "torch.save(model.state_dict(), f'{DATA_DIR}len_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b682d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_reporter = len_model.fit(len_dataloader, val_dataset=len_val_dataset, embedder=embedder), epochs=epochs, lr=lr)\n",
    "# Save the Reporter object using pickle.\n",
    "with open(f'{DATA_DIR}len_reporter.pkl', 'wb') as f:\n",
    "        pickle.dump(train_reporter, f)\n",
    "# Save the weights of the trained model. \n",
    "torch.save(model.state_dict(), f'{DATA_DIR}len_model_weights.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14bfb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "len_reporter = len_model.fit(len_dataloader, val_dataset=len_val_dataset, embedder=embedder), epochs=epochs, lr=lr)\n",
    "# Save the Reporter object using pickle.\n",
    "with open(f'{DATA_DIR}len_reporter.pkl', 'wb') as f:\n",
    "        pickle.dump(train_reporter, f)\n",
    "# Save the weights of the trained model. \n",
    "torch.save(model.state_dict(), f'{DATA_DIR}len_model_weights.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69db1b18",
   "metadata": {},
   "source": [
    "## Evaluating performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b7d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download(filename:str, directory:str, config:ConfigParser, stream:bool=False) -> ConfigParser:\n",
    "    '''Download a file from the Google Cloud bucket, writing the information to the specified directory.\n",
    "    Also adds the resulting filepath to the config file.\n",
    "    \n",
    "    args:\n",
    "        - filename: The name of the file in the Google Cloud bucket.\n",
    "        - directory: The directory location where the downloaded file will be stored. \n",
    "        - config: The configuration file object.\n",
    "        - stream: Whether or not to stream the downloaded file to avoid excessive RAM usage. \n",
    "    '''\n",
    "    path = os.path.join(directory, filename)\n",
    "    if not os.path.exists(path): # Skip the download step if file is already present. \n",
    "        t1 = perf_counter()\n",
    "        response = requests.get(BUCKET + filename, stream=stream)\n",
    "        path = write_file(path, response, stream=stream)\n",
    "        t2 = perf_counter()\n",
    "        print(f'setup.main.download: Downloaded {filename} to {directory} in {np.round(t2 - t1, 2)} seconds.')\n",
    "        \n",
    "    key = filename.split('.')[0] + '_path' # Create a key for the path in the config file by removing the file extension.\n",
    "    config['paths'][key] = path # This should be the extracted file's path, when unzip=True.\n",
    "    return config\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
