{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# I want to assess how much length affects the effect of sequence length alone on the different embedding approaches. Really, I want\n",
    "# to see how much the mean-pooling approach alters things. This is difficult to determine by looking at the embeddings of short\n",
    "# versus long sequences, as length will also be connected with the protein family (and therefore the amino acid composition, ordering, etc.) It's \n",
    "# also hard to compare the two models, as the embedding dimensions are so different. \n",
    "\n",
    "# The best thing I can think to do is compare ESM's CLS token with the mean-pooled ESM last hidden layer. This might be the closest\n",
    "# thing to an apples-to-apples comparison I can get. \n",
    "\n",
    "import pandas as pd \n",
    "from transformers import T5Tokenizer, T5EncoderModel, AutoTokenizer, EsmModel\n",
    "from tqdm import tqdm \n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = EsmModel.from_pretrained('facebook/esm2_t33_650M_UR50D')\n",
    "model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/esm2_t33_650M_UR50D')\n",
    "\n",
    "metadata_df = pd.read_csv('../data/test_metadata.csv', index_col=0)\n",
    "metadata_df['length'] = metadata_df.seq.apply(len)\n",
    "metadata_df = metadata_df[metadata_df.length < 500] # Stick with smaller sequences so it's less intensive. \n",
    "metadata_df = metadata_df.sample(200)\n",
    "\n",
    "seqs = metadata_df.seq.values.tolist()\n",
    "\n",
    "embs = {'mean_pooled':[], 'cls_token':[]}\n",
    "\n",
    "for seq in tqdm(seqs, 'Embedding sequences...'):\n",
    "    inputs = tokenizer([seq])\n",
    "    inputs = {k:torch.tensor(v).to(device) for k, v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    embs['cls_token'].append(outputs.last_hidden_state[:, 0].cpu())\n",
    "    embs['mean_pooled'].append(outputs.last_hidden_state.mean(dim=1).cpu())\n",
    "\n",
    "store = pd.HDFStore('subset.h5', 'w')\n",
    "store.put('metadata', metadata_df)\n",
    "for key, data in embs.items():\n",
    "    print(f'Saving {key} embeddings.')\n",
    "    df = torch.cat(data).float().numpy()\n",
    "    df = pd.DataFrame(df, index=metadata_df.index)\n",
    "    store.put(key, df)\n",
    "store.close()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
