{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "In order to further validate the promising results of the PLM classifier on the testing data, we sought to interrogate a well-characterized microbial genome using the model. To this end, we chose to analyze the complete genome of Escherichia coli (strain K-12, substrain. MG1655, assembly ASM584v2). The RefSeq accession for this genome is GCF_000005845.2. \n",
    "\n",
    "For organizational purposes, all data used for model validation is placed into a subdirectory within the data directory. If you want to run this code, be sure to modify the `DATA_DIR` variable below to specify where the data will be stored on your machine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the selenobot/ subirectory to the module search path, so that the modules in this directory are visible from the notebook.\n",
    "sys.path.append('../selenobot/')\n",
    "\n",
    "from dataset import Dataset\n",
    "from classifiers import Classifier\n",
    "from utils import csv_size, dataframe_from_gff\n",
    "from extend import extend\n",
    "import pandas as pd\n",
    "import re\n",
    "from typing import NoReturn\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/prichter/Documents/data/selenobot-test/validation/'\n",
    "\n",
    "genome_id = 'GCF_000005845.2' # The accession of the genome to download. \n",
    "assembly = 'ASM584v2' # The specific genome assembly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading genome data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2655k    0 2655k    0     0   313k      0 --:--:--  0:00:08 --:--:--  340k\n",
      "Archive:  /home/prichter/Documents/data/selenobot-test/validation/ncbi_dataset.zip\n",
      "  inflating: /home/prichter/Documents/data/selenobot-test/validation/README.md  \n",
      "  inflating: /home/prichter/Documents/data/selenobot-test/validation/ncbi_dataset/data/assembly_data_report.jsonl  \n",
      "  inflating: /home/prichter/Documents/data/selenobot-test/validation/ncbi_dataset/data/GCF_000005845.2/GCF_000005845.2_ASM584v2_genomic.fna  \n",
      "  inflating: /home/prichter/Documents/data/selenobot-test/validation/ncbi_dataset/data/GCF_000005845.2/protein.faa  \n",
      "  inflating: /home/prichter/Documents/data/selenobot-test/validation/ncbi_dataset/data/GCF_000005845.2/genomic.gff  \n",
      "  inflating: /home/prichter/Documents/data/selenobot-test/validation/ncbi_dataset/data/dataset_catalog.json  \n"
     ]
    }
   ],
   "source": [
    "# Download and unzip the genome data from NCBI. \n",
    "! curl 'https://api.ncbi.nlm.nih.gov/datasets/v2alpha/genome/accession/{genome_id}/download?include_annotation_type=GENOME_FASTA,PROT_FASTA,GENOME_GFF' -o '{DATA_DIR}ncbi_dataset.zip'\n",
    "! unzip '{DATA_DIR}ncbi_dataset.zip' -d '{DATA_DIR}'\n",
    "\n",
    "# Create a directory to store the genome files. \n",
    "! mkdir '{DATA_DIR}{genome_id}/' \n",
    "# Move the relevant files into the new directory for organizational purposes. \n",
    "! mv '{DATA_DIR}ncbi_dataset/data/{genome_id}/genomic.gff' -t '{DATA_DIR}{genome_id}/'\n",
    "! mv '{DATA_DIR}ncbi_dataset/data/{genome_id}/protein.faa' -t '{DATA_DIR}{genome_id}/'\n",
    "! mv '{DATA_DIR}ncbi_dataset/data/{genome_id}/{genome_id}_{assembly}_genomic.fna' '{DATA_DIR}{genome_id}/genome.fna'\n",
    "\n",
    "# Remove some extraneous files which were also downloaded. \n",
    "! rm '{DATA_DIR}README.md'\n",
    "! rm -R '{DATA_DIR}ncbi_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(path:str) -> NoReturn:\n",
    "    '''Modify the format of downloaded FASTA files to standardize reading and writing FASTA files.'''\n",
    "    fasta = ''\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    # In the FASTA files downloaded from NCBI, the only relevant information is right after the > character.\n",
    "    # This is the GenBank protein accession, which will be called the 'id'.\n",
    "    for line in lines:\n",
    "        if '>' in line: # This symbol marks the beginning of a header file.\n",
    "            id_ = re.search('>([^\\s]+)', line).group(1)\n",
    "            fasta += f'>id={id_}\\n'\n",
    "        else:\n",
    "            fasta += line\n",
    "    # This will overwrite the original file. \n",
    "    with open(path, 'w') as f:\n",
    "        f.write(fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean(f'{DATA_DIR}{genome_id}/protein.faa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting selenoproteins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating PLM embeddings\n",
    "\n",
    "Using the same procedure as was used to create embeddings for the UniProt sequences for training the classifier, embeddings are generated for all predicted genes in the model genome. The code for embedding sequence data is provided in the `embed.py` file in the `scripts` directory. The process for generating embeddings was computationally-intensive, and had to be run on an external computer cluster. The steps of the embedding algorithm, which is implemented by the `PlmEmbedder` class, are given below.\n",
    "\n",
    "1. **Amino acid sequences are read from a FASTA file.**\n",
    "2. **All non-standard amino acids were replaced with an “X”.**\n",
    "3. **Sequences were sorted in ascending order according to length.** This avoids the addition of unnecessary padding. \n",
    "4. **The sequences are processed and tokenized in batches.** Processing sequences in batches enables one to generate the embeddings as quickly as possible, while also preventing the GPUs from crashing. The maximum number of sequences in any given batch is 100, the maximum number of amino acids in a batch is 4000, and any sequence longer than 1000 amino acids is processed individually. \n",
    "5. **The PLM is used to generate embeddings.** These embeddings have shape shape `(length, latent dimension)`.\n",
    "6. **Each embedding is sliced according to the length of the original sequence.** This is due to the fact that part of the model output corresponds to the padding tokens, which should be excluded from the final embedding. \n",
    "7. **The embeddings are mean-pooled over sequence length.** This step standardizes the length of the embedding vectors to be of fixed dimension (the latent dimension of the PLM), which is necessary for passing them to the Selenobot linear classifier. \n",
    "The output of the steps above is data containing gene IDs, the original amino acid sequences (which may contain non-standard residues), and columns with the mean-pooled embeddings. \n",
    "\n",
    "There are several pre-embedded genomes available in the Google Cloud repository, which can be downloaded using the code below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the PLM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataframe_from_fasta(f'{DATA_DIR}protein.faa')\n",
    "dataset = Dataset(df, embedder=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Classifier(latent_dim=1024, hidden_dim=512)\n",
    "model.load_state_dict(torch.load(f'{DATA_DIR}plm_model_weights.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_genome(path:str) -> str:\n",
    "    '''Load in the complete nucleotide sequence of the genome.\n",
    "    \n",
    "    :param path: A FASTA file from NCBI which contains a complete genome. \n",
    "    :return: A string of nucleotides. \n",
    "    '''\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.read().splitlines()[1:] # Skip the header line. \n",
    "        seq = ''.join(lines)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually, will need to be able to support this for a whole list of genomes.\n",
    "def database_build_query() -> NoReturn:\n",
    "    '''Build a query data database, which contains the sequences to search for homology matches for.'''\n",
    "    # Grab the coordinate information about the predicted selenoproteins only. Exclude known selenoproteins.\n",
    "    database = load_coordinates(gene_ids=[g for g in load_predictions() if g not in known_selenoproteins])\n",
    "    # Mark the sequences which will be extended past the first STOP codon. \n",
    "    # database['extend'] = [(gene_id not in known_selenoproteins) for gene_id in database.gene_id]\n",
    "    database['extend'] = True \n",
    "    database = get_sequences(database, load_genome('', path=os.path.join(DATA_DIR, 'genome.fasta')))\n",
    "    database_write(database, filename='query.fasta')\n",
    "\n",
    "\n",
    "def database_build_control() -> NoReturn:\n",
    "    '''Build a control query data database, which contains the non-extended selenoprotein sequences.'''\n",
    "    # Grab the coordinate information about the predicted selenoproteins only. Exclude known selenoproteins.\n",
    "    database = load_coordinates(gene_ids=[id_ for id_ in load_predictions() if id_ not in known_selenoproteins])\n",
    "    database['extend'] = False # Don't extend anything here. \n",
    "    database = get_sequences(database, load_genome('', path=os.path.join(DATA_DIR, 'genome.fasta')))\n",
    "    database_write(database, filename='control.fasta')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///home/prichter/Documents/find-a-bug-api\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from Find-A-Bug-API==0.0.0) (2.31.0)\n",
      "Requirement already satisfied: pandas in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from Find-A-Bug-API==0.0.0) (2.1.1)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from pandas->Find-A-Bug-API==0.0.0) (1.26.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from pandas->Find-A-Bug-API==0.0.0) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from pandas->Find-A-Bug-API==0.0.0) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from pandas->Find-A-Bug-API==0.0.0) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from requests->Find-A-Bug-API==0.0.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from requests->Find-A-Bug-API==0.0.0) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from requests->Find-A-Bug-API==0.0.0) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from requests->Find-A-Bug-API==0.0.0) (2023.11.17)\n",
      "Requirement already satisfied: six>=1.5 in /home/prichter/miniconda3/envs/selenobot/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->Find-A-Bug-API==0.0.0) (1.16.0)\n",
      "Installing collected packages: Find-A-Bug-API\n",
      "  Attempting uninstall: Find-A-Bug-API\n",
      "    Found existing installation: Find-A-Bug-API 0.0.0\n",
      "    Uninstalling Find-A-Bug-API-0.0.0:\n",
      "      Successfully uninstalled Find-A-Bug-API-0.0.0\n",
      "  Running setup.py develop for Find-A-Bug-API\n",
      "Successfully installed Find-A-Bug-API-0.0.0\n"
     ]
    }
   ],
   "source": [
    "! pip install -e /home/prichter/Documents/find-a-bug-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from align import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import fabapi\n",
    "import fabapi.genomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
