{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set-up\n",
    "\n",
    "This notebook describes the set-up procedure for the training, test, and validation datasets used to produce the trained Selenobot model The three files created during set-up are `train.csv`, `test.csv`, and `val.csv`, which contain the training, testing, and validation data, respectively. Each file contains the following fields:\n",
    "1. `gene_id` A unique identifier for the amino acid sequence. \n",
    "2. `seq` An amino acid sequence. \n",
    "3. `label` Either 1 (indicating a truncated selenoprotein) or 0 (indicating a full-length non-selenoprotein)\n",
    "4. `0 ... 1024` The mean-pooled PLM embedding vector. \n",
    " \n",
    "**NOTE:** The steps described in this notebook are not necessary for replicating Selenobot results. Pre-trained models, as well as completed training, testing, and validation datasets are available in a Google Cloud bucket, and instructions for downloading them can be found in the `testing.ipynb` and `training.ipynb` notebooks.\n",
    "\n",
    "If you want to run this code, be sure to modify the `DATA_DIR` and `CDHIT` variables below to specify where the data will be stored on your machine. `DATA_DIR` is the absolute path specifying the location where the data will be stored, and `CDHIT` is the absolute path to the CD-HIT command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/prichter/Documents/data/selenobot-test/'\n",
    "CDHIT = '/home/prichter/cd-hit-v4.8.1-2019-0228/cd-hit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Add the selenobot/ subirectory to the module search path, so that the utils module is visible from the notebook.\n",
    "sys.path.append('../selenobot/')\n",
    "\n",
    "from utils import dataframe_from_fasta, dataframe_to_fasta, dataframe_from_clstr, fasta_size, csv_size # Some functions for reading and writing FASTA files. \n",
    "import pandas as pd\n",
    "from typing import NoReturn, Tuple\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading UniProt data\n",
    "\n",
    "The sequence data used to construct the training, test, and validation sets was obtained from Uniprot (release 2023_03, accessed 08/11/2023). The downloaded sequences were (1) all known selenoproteins in the entirety of UniProt, and (2) all SwissProt-reviewed full-length proteins. The selenoproteins can be obtained through the SwissProt REST API using the URL below. The SwissProt sequences can be downloaded as a zip file directly from the UniProt website, which can be extracted using `gzip`.\n",
    "\n",
    "**NOTE:** Downloading the UniProt data took over an hour to run when I tried this, so don't worry if it's taking a while!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download SwissProt from UniProt (release 2023_03). \n",
    "# ! curl -s 'https://ftp.uniprot.org/pub/databases/uniprot/previous_releases/release-2023_03/knowledgebase/uniprot_sprot-only2023_03.tar.gz' -o '{DATA_DIR}sprot.fasta.tar.gz'\n",
    "# Download known selenoproteins from UniProt. Filter for sequences added before the date we accessed the database (August 11, 2023)\n",
    "! curl -s 'https://rest.uniprot.org/uniprotkb/stream?format=fasta&query=%28%28ft_non_std%3Aselenocysteine%29+AND+%28date_created%3A%5B*+TO+2023-08-11%5D%29%29' -o '{DATA_DIR}sec.fasta'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip the SwissProt sequence file. \n",
    "! tar -xf '{DATA_DIR}sprot.fasta.tar.gz' -C '{DATA_DIR}'\n",
    "! gunzip -d '{DATA_DIR}uniprot_sprot.fasta.gz'\n",
    "! mv '{DATA_DIR}uniprot_sprot.fasta' '{DATA_DIR}sprot.fasta'\n",
    "\n",
    "# Clean up some of the extra files which were extracted along with the SwissProt FASTA file.\n",
    "! rm '{DATA_DIR}uniprot_sprot.dat.gz'\n",
    "! rm '{DATA_DIR}uniprot_sprot_varsplic.fasta.gz'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(path:str) -> NoReturn:\n",
    "    '''Modify the format of downloaded FASTA files to standardize reading and writing FASTA files.'''\n",
    "    fasta = ''\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        if '>' in line: # This symbol marks the beginning of a header file.\n",
    "            id_ = re.search('\\|([\\w\\d_]+)\\|', line).group(1)\n",
    "            fasta += f'>id={id_}\\n'\n",
    "        else:\n",
    "            fasta += line\n",
    "    # This will overwrite the original file. \n",
    "    with open(path, 'w') as f:\n",
    "        f.write(fasta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This reformats the headers in the FASTA files to standardize reading and writing\n",
    "# from FASTA files across the Selenobot project. \n",
    "clean(f'{DATA_DIR}sprot.fasta')\n",
    "clean(f'{DATA_DIR}sec.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Removing selenoproteins\n",
    "\n",
    "The SwissProt file contains some known selenoproteins which passed the review process. However, if we leave them in the SwissProt file, they will not be truncated and labeled as selenoproteins in later steps. This might cause some issues during the training process, as we would potentially bias the classifier against flagging the truncated equivalents. To avoid these problems, we remove all selenoproteins from the `sprot.fasta` file using the function below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_selenoproteins(path:str) -> NoReturn:\n",
    "    '''Remove all selenoproteins which are present in the SwissProt download.'''\n",
    "    df = dataframe_from_fasta(path) # Load the SwissProt data into a pandas DataFrame.\n",
    "    selenoproteins = df['seq'].str.contains('U') # Determine the indices where the selenoproteins occur. \n",
    "    if np.sum(selenoproteins) > 0:\n",
    "        df = df[~selenoproteins]\n",
    "        print(f'{np.sum(selenoproteins)} selenoproteins successfully removed from SwissProt.')\n",
    "        dataframe_to_fasta(df, path=path) # Overwrite the original file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_selenoproteins(f'{DATA_DIR}sprot.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncating selenoproteins\n",
    "\n",
    "The goal of the Selenobot classifier is to distinguish a truncated selenoprotein from a full-length non-selenoprotein, so all selenoproteins present in the training, testing, and validation data should be truncated. Although some selenoproteins contain multiple selenocysteine residues, we chose to truncate at the first selenocysteine residue only. This choice was made because there are no selenoproteins identified in GTDB, to which we will ultimately apply the trained model. So there cannot be any instances of selenoproteins truncated at the second, third, etc. selenocysteine residue\n",
    "\n",
    "The `truncate_selenoproteins` function defined below truncates all selenoproteins in the input file (`sec.fasta`). It also appends a  `\"[1]”`` to the gene IDs of truncated proteins, indicating truncation at the first selenocysteine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_selenoproteins(in_path:str, out_path:str) -> NoReturn:\n",
    "    '''Truncate the selenoproteins stored in the input file. This function assumes that all \n",
    "    sequences contained in the file contain selenocysteine, labeled as U.'''\n",
    "    # Load the selenoproteins into a pandas DataFrame. \n",
    "    df = dataframe_from_fasta(in_path)\n",
    "    df_trunc = {'id':[], 'seq':[]}\n",
    "    for row in df.itertuples():\n",
    "        df_trunc['id'].append(row.id + '[1]') # Modify the row ID to contain \n",
    "        df_trunc['seq'].append(row.seq)\n",
    "    df = pd.DataFrame(df_trunc)\n",
    "    dataframe_to_fasta(df, path=out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncate_selenoproteins(f'{DATA_DIR}sec.fasta', f'{DATA_DIR}sec_truncated.fasta')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining files\n",
    "\n",
    "To make the UniProt data easier to work with in later steps (specifically, for using the CD-HIT clustering tool), we concatenate the data in the `sec_truncated.fasta` and `sprot.fasta` files into a single `uniprot.fasta` file. This operation can be accomplished in the terminal using the `cat` command. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the FASTA files in the data directory. \n",
    "! cat '{DATA_DIR}sec_truncated.fasta' '{DATA_DIR}sprot.fasta' > '{DATA_DIR}uniprot.fasta'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading PLM embeddings\n",
    "\n",
    "Embeddings were generated using a version of the Prot-T5 pre-trained protein language model entitled `Rostlab/prot_t5_xl_half_uniref50-enc`. This model is the encoder portion of the 3 billion-parameter model, trained using a masking approach. The model weights were obtained from HuggingFace via the transformers Python library. \n",
    "\n",
    "The code for embedding sequence data is provided in the `embed.py` file in the `scripts` directory. The process for generating embeddings was computationally-intensive, and had to be run on an external computer cluster. The steps of the embedding algorithm, which is implemented by the `PlmEmbedder` class, are given below.\n",
    "1. **Amino acid sequences are read from a FASTA file.**\n",
    "2. **All non-standard amino acids were replaced with an “X”.**\n",
    "3. **Sequences were sorted in ascending order according to length.** This avoids the addition of unnecessary padding. \n",
    "4. **The sequences are processed and tokenized in batches.** Processing sequences in batches enables one to generate the embeddings as quickly as possible, while also preventing the GPUs from crashing. The maximum number of sequences in any given batch is 100, the maximum number of amino acids in a batch is 4000, and any sequence longer than 1000 amino acids is processed individually. \n",
    "5. **The PLM is used to generate embeddings.** These embeddings have shape shape `(length, latent dimension)`.\n",
    "6. **Each embedding is sliced according to the length of the original sequence.** This is due to the fact that part of the model output corresponds to the padding tokens, which should be excluded from the final embedding. \n",
    "7. **The embeddings are mean-pooled over sequence length.** This step standardizes the length of the embedding vectors to be of fixed dimension (the latent dimension of the PLM), which is necessary for passing them to the Selenobot linear classifier. \n",
    "The output of the steps above is data containing gene IDs, the original amino acid sequences (which may contain non-standard residues), and columns with the mean-pooled embeddings. \n",
    "\n",
    "This process was applied to every sequence in the `uniprot.fasta`, producing the `embeddings.csv` file. This file is available for download in a [Google Cloud Bucket](https://storage.googleapis.com/selenobot-data/embeddings.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 7122M  100 7122M    0     0  27.3M      0  0:04:20  0:04:20 --:--:-- 25.7M02  0:00:45  0:03:17 26.7M\n"
     ]
    }
   ],
   "source": [
    "# Download the PLM embeddings from the Google Cloud Bucket\n",
    "! curl 'https://storage.googleapis.com/selenobot-data/embeddings.csv' -o '{DATA_DIR}embeddings.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering with CD-HIT\n",
    "\n",
    "When using sequence data to train machine learning models, it is important to control for homology when constructing the training, testing, and validation sets. A failure to do so may result in data leakage, as the model will learn evolutionary relationships instead of more relevant sequence characteristics (e.g. truncation, in the case of Selenobot). To control for sequence homology, we first processed the data in uniprot.fasta using CD-HIT, a widely used program for clustering biological sequences. The clustering parameters used are as follows.\n",
    "\n",
    "1. **c=0.8** This sets the sequence similarity threshold. In other words, sequences which are given a similarity score of 0.8 or higher are grouped into the same cluster. \n",
    "2. **l=5** This parameter specifies minimum sequence length. Setting this to 5 means that no sequences fewer than amino acids in length are clustered. This is the minimum length allowed by CD-HIT. Some of the truncated selenoprotein did not meet this length requirement, and were discarded. \n",
    "3. **n=5** This is the “word length” used by the CD-HIT algorithm to determine sequence similarity. This parameter was kept as the default, which is the recommended word length when using a sequence similarity threshold of 0.8. \n",
    "\n",
    "The CD-HIT output is a `uniprot.clstr` file, which maps gene IDs to the ID of the cluster to which they belong. 278931 clusters were generated by the program when it was run for our investigation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================\n",
      "Program: CD-HIT, V4.8.1 (+OpenMP), Aug 24 2023, 13:02:16\n",
      "Command: /home/prichter/cd-hit-v4.8.1-2019-0228/cd-hit -i\n",
      "         /home/prichter/Documents/data/selenobot-test/uniprot.fasta\n",
      "         -o\n",
      "         /home/prichter/Documents/data/selenobot-test/uniprot\n",
      "         -n 5 -c 0.8 -l 5\n",
      "\n",
      "Started: Sat Feb  3 18:41:44 2024\n",
      "================================================================\n",
      "                            Output                              \n",
      "----------------------------------------------------------------\n",
      "total seq: 588641\n",
      "longest and shortest : 35213 and 6\n",
      "Total letters: 215202198\n",
      "Sequences have been sorted\n",
      "\n",
      "Approximated minimal memory consumption:\n",
      "Sequence        : 288M\n",
      "Buffer          : 1 X 25M = 25M\n",
      "Table           : 1 X 74M = 74M\n",
      "Miscellaneous   : 7M\n",
      "Total           : 395M\n",
      "\n",
      "Table limit with the given memory limit:\n",
      "Max number of representatives: 674635\n",
      "Max number of word counting entries: 50531311\n",
      "\n",
      "comparing sequences from          0  to      41986\n",
      "..........    10000  finished       6421  clusters\n",
      "..........    20000  finished      12133  clusters\n",
      "..........    30000  finished      17085  clusters\n",
      "..........    40000  finished      22812  clusters\n",
      "comparing sequences from      41986  to     217835\n",
      "..........    50000  finished      28399  clusters\n",
      "..........    60000  finished      33327  clusters\n",
      "..........    70000  finished      38636  clusters\n",
      "..........    80000  finished      43763  clusters\n",
      "..........    90000  finished      49157  clusters\n",
      "..........   100000  finished      53974  clusters\n",
      "..........   110000  finished      59291  clusters\n",
      "..........   120000  finished      63903  clusters\n",
      "..........   130000  finished      69001  clusters\n",
      "..........   140000  finished      73610  clusters\n",
      "..........   150000  finished      78637  clusters\n",
      "..........   160000  finished      83395  clusters\n",
      "..........   170000  finished      87962  clusters\n",
      "..........   180000  finished      92807  clusters\n",
      "..........   190000  finished      97721  clusters\n",
      "..........   200000  finished     102369  clusters\n",
      "..........   210000  finished     106528  clusters\n",
      "comparing sequences from     217835  to     588641\n",
      "..........   220000  finished     111485  clusters\n",
      "..........   230000  finished     116259  clusters\n",
      "..........   240000  finished     121239  clusters\n",
      "..........   250000  finished     126042  clusters\n",
      "..........   260000  finished     130789  clusters\n",
      "..........   270000  finished     135500  clusters\n",
      "..........   280000  finished     140195  clusters\n",
      "..........   290000  finished     145152  clusters\n",
      "..........   300000  finished     149939  clusters\n",
      "..........   310000  finished     154825  clusters\n",
      "..........   320000  finished     159780  clusters\n",
      "..........   330000  finished     164683  clusters\n",
      "..........   340000  finished     169626  clusters\n",
      "..........   350000  finished     174433  clusters\n",
      "..........   360000  finished     179145  clusters\n",
      "..........   370000  finished     183823  clusters\n",
      "..........   380000  finished     188876  clusters\n",
      "..........   390000  finished     193737  clusters\n",
      "..........   400000  finished     198468  clusters\n",
      "..........   410000  finished     203116  clusters\n",
      "..........   420000  finished     207735  clusters\n",
      "..........   430000  finished     212607  clusters\n",
      "..........   440000  finished     217208  clusters\n",
      "..........   450000  finished     221733  clusters\n",
      "..........   460000  finished     225913  clusters\n",
      "..........   470000  finished     230352  clusters\n",
      "..........   480000  finished     234826  clusters\n",
      "..........   490000  finished     238954  clusters\n",
      "..........   500000  finished     243083  clusters\n",
      "..........   510000  finished     246911  clusters\n",
      "..........   520000  finished     251505  clusters\n",
      "..........   530000  finished     255557  clusters\n",
      "..........   540000  finished     259706  clusters\n",
      "..........   550000  finished     263854  clusters\n",
      "..........   560000  finished     268076  clusters\n",
      "..........   570000  finished     272301  clusters\n",
      "..........   580000  finished     276179  clusters\n",
      "........\n",
      "   588641  finished     279559  clusters\n",
      "\n",
      "Approximated maximum memory consumption: 745M\n",
      "writing new database\n",
      "writing clustering information\n",
      "program completed !\n",
      "\n",
      "Total CPU time 211.17\n"
     ]
    }
   ],
   "source": [
    "! {CDHIT} -i '{DATA_DIR}uniprot.fasta' -o '{DATA_DIR}uniprot' -n 5 -c 0.8 -l 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning by cluster\n",
    "\n",
    "We chose the size of the training dataset to be 80 percent of `uniprot.fasta`. Roughly 60 percent of remaining sequences were then sorted into the testing dataset, and the leftover sequences were reserved for the validation dataset. To ensure that no sequences which belong to the same CD-HIT cluster are present in separate datasets, we define a custom `sample` function which uses the information in `uniprot.clstr` to ensure that the entirety of any homology group is contained within the sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Approximate size of training dataset: 470970\n",
      "Approximate size of testing dataset: 70645\n",
      "Approximate size of validation dataset: 47098\n"
     ]
    }
   ],
   "source": [
    "train_size = int(0.8 * fasta_size(f'{DATA_DIR}uniprot.fasta'))\n",
    "test_size = int(0.6 * (fasta_size(f'{DATA_DIR}uniprot.fasta') - train_size))\n",
    "val_size = fasta_size(f'{DATA_DIR}uniprot.fasta') - (train_size + test_size)\n",
    "\n",
    "print('Approximate size of training dataset:', train_size)\n",
    "print('Approximate size of testing dataset:', test_size)\n",
    "print('Approximate size of validation dataset:', val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(df:pd.DataFrame, n:int=None) -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    '''Sample from the cluster data such that the entirety of any homology group is contained in the sample. This function assumes that\n",
    "    the size of the sample is smaller than half of the size of the input DataFrame. \n",
    "  \n",
    "    :param df: A pandas DataFrame mapping the gene ID to cluster number. \n",
    "    :param n: The size of the sample. \n",
    "    :return: A tuple of DataFrames, the first being the sample, and the second being the input data with the sample removed. \n",
    "    '''\n",
    "    assert (len(df) - n) >= n, f'The sample size must be less than half of the input data.'\n",
    "\n",
    "    groups = {'sample':[], 'remainder':[]}\n",
    "    curr_size = 0 # Keep track of the sample size. \n",
    "    ordered_clusters = df.groupby('cluster').size().sort_values(ascending=False).index # Sort the clusters in descending order of size. \n",
    "\n",
    "    add_to = 'sample'\n",
    "    for cluster in tqdm(ordered_clusters, desc='sample'): # Iterate over the cluster IDs. \n",
    "        cluster = df[df.cluster == cluster] # Grab an entire cluster from the DataFrame. \n",
    "        if add_to == 'sample' and curr_size < n: # Only add to the sample partition while the size requirement is not yet met. \n",
    "            groups['sample'].append(cluster)\n",
    "            curr_size += len(cluster)\n",
    "            add_to = 'remainder'\n",
    "        else:\n",
    "            groups['remainder'].append(cluster)\n",
    "            add_to = 'sample'\n",
    "\n",
    "    sample, remainder = pd.concat(groups['sample']), pd.concat(groups['remainder'])\n",
    "    # Some final checks to make sure the sampling function behaved as expected. \n",
    "    assert len(sample) + len(remainder) == len(df), f'The combined sizes of the partitions do not add up to the size of the original data.'\n",
    "    assert len(sample) < len(remainder), f'The sample DataFrame should be smaller than the remainder DataFrame.'\n",
    "    return sample, remainder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 sequences were not assigned cluster groups and were dropped.\n"
     ]
    }
   ],
   "source": [
    "uniprot_df = dataframe_from_fasta(f'{DATA_DIR}uniprot.fasta') # Read in the UniProt data.\n",
    "clstr_df = dataframe_from_clstr(f'{DATA_DIR}uniprot.clstr') # Read in the cluster data generated by CD-HIT\n",
    "\n",
    "uniprot_df = uniprot_df.merge(clstr_df, on='id') # Add the cluster labels to the UniProt data. \n",
    "print(fasta_size(f'{DATA_DIR}uniprot.fasta') - len(uniprot_df), 'sequences were not assigned cluster groups and were dropped.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 279559/279559 [02:49<00:00, 1649.35it/s]\n",
      "sample: 100%|██████████| 10219/10219 [00:03<00:00, 2686.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of training dataset: 470969\n",
      "Size of testing dataset: 70571\n",
      "Size of validation dataset: 47101\n"
     ]
    }
   ],
   "source": [
    "uniprot_df, train_df = sample(uniprot_df, n=len(uniprot_df) - train_size)\n",
    "val_df, test_df = sample(uniprot_df, n=val_size)\n",
    "\n",
    "print('Size of training dataset:', len(train_df))\n",
    "print('Size of testing dataset:', len(test_df))\n",
    "print('Size of validation dataset:', len(val_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset is written to a CSV file (`train.csv`, `test.csv`, and `val.csv`) in the data directory. The datasets now contain the following fields:\n",
    "1. **id** The gene ID from UniProt. The truncated selenoproteins have a “[1]\" appended to their ID, which was added when the `truncate_selenoproteins` function was called.\n",
    "2. **seq** The amino acid sequence.\n",
    "3. **cluster** The ID of the cluster to which the sequence belongs. This field is not used after the partitioning step, but is kept in the data for the sake of completeness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.set_index('id').to_csv(f'{DATA_DIR}train.csv')\n",
    "test_df.set_index('id').to_csv(f'{DATA_DIR}test.csv')\n",
    "val_df.set_index('id').to_csv(f'{DATA_DIR}val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding labels and embeddings\n",
    "\n",
    "The final step required to set up the training, testing, and validation datasets is to add labels and PLM embeddings to the files. For labels, we simply add a “labels” field, which contains either a `1` (indicating a truncated selenoprotein) or `0` (indicating a full-length non-selenoprotein). These labels are added according to whether or not a “[1]” is present in the ID for a particular sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in [f'{DATA_DIR}train.csv', f'{DATA_DIR}test.csv', f'{DATA_DIR}val.csv']:\n",
    "    df = pd.read_csv(path)\n",
    "    # Add a label column, marking those sequences with a [1] in the gene ID with a 1.\n",
    "    # Make sure to turn off refex for the str.contains function, or it will try to match [1] as a pattern.\n",
    "    df['label'] = df['id'].str.contains('[1]', regex=False).astype(int)\n",
    "    df.set_index('id').to_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding the embeddings to the datasets is slightly more involved, as the entire `embeddings.csv` file is too large to load into memory. So, we processed the embeddings in chunks using the `add_embeddings` function below. First, we load all gene IDs present in the embeddings.csv file. Then, we read the dataset (either `train.csv`, `test.csv`, or `val.csv`) in chunks of size 1000. We use the vector of gene IDs from the file to load in only those rows in `embeddings.csv` whose gene IDs are contained in the dataset chunk. We then add these embeddings to the dataset chunk, and write the chunk (with the embeddings added) to a temporary CSV file. This process is repeated, and dataset chunks are appended to the temporary CSV file until all embeddings have been added to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_embeddings(path:str, chunk_size:int=1000) -> NoReturn:\n",
    "    '''Add embedding information to a dataset, and overwrite the original dataset with the\n",
    "    modified dataset (with PLM embeddings added).\n",
    "    \n",
    "    :param path: The path to the dataset.\n",
    "    :chunk_size: The size of the chunks to split the dataset into for processing.\n",
    "    '''\n",
    "    embedding_ids = pd.read_csv(f'{DATA_DIR}embeddings.csv', usecols=['id'])['id'].values.ravel() # Read the IDs in the embedding file to avoid loading the entire thing into memory.\n",
    "    reader = pd.read_csv(path, index_col=['id'], chunksize=chunk_size) # Use read_csv to load the dataset one chunk at a time. \n",
    "    tmp_file_path = f'{DATA_DIR}tmp.csv' # The path to the temporary file to which the modified dataset will be written in chunks.\n",
    "\n",
    "    is_first_chunk = True\n",
    "    n_chunks = csv_size(path) // chunk_size + 1\n",
    "    for chunk in tqdm(reader, desc='add_embeddings', total=n_chunks):\n",
    "        # Get the indices of the embedding rows corresponding to the data chunk. Make sure to shift the index up by one to account for the header. \n",
    "        idxs = np.where(np.isin(embedding_ids, chunk.index, assume_unique=True))[0] + 1 \n",
    "        idxs = [0] + list(idxs) # Add the header index so the column names are included. \n",
    "        # Read in the embedding rows, skipping rows which do not match a gene ID in the chunk. \n",
    "        chunk = chunk.merge(pd.read_csv(f'{DATA_DIR}embeddings.csv', skiprows=lambda i : i not in idxs), on='id', how='inner')\n",
    "        # Check to make sure the merge worked as expected. Subtract 1 from len(idxs) to account for the header row.\n",
    "        assert len(chunk) == (len(idxs) - 1), f'Data was lost while merging embedding data.'\n",
    "        \n",
    "        chunk.to_csv(tmp_file_path, header=is_first_chunk, mode='w' if is_first_chunk else 'a') # Only write the header for the first file. \n",
    "        is_first_chunk = False\n",
    "    # Replace the old dataset with the temporary file. \n",
    "    subprocess.run(f'rm {path}', shell=True, check=True)\n",
    "    subprocess.run(f'mv {tmp_file_path} {path}', shell=True, check=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "setup.data.detect.add_embeddings_to_file: 100%|██████████| 471/471 [16:03:44<00:00, 122.77s/it]      \n",
      "setup.data.detect.add_embeddings_to_file: 100%|██████████| 71/71 [50:34<00:00, 42.74s/it]\n",
      "setup.data.detect.add_embeddings_to_file: 100%|██████████| 48/48 [34:03<00:00, 42.57s/it]\n"
     ]
    }
   ],
   "source": [
    "# Add the embedding data to each dataset.\n",
    "add_embeddings(f'{DATA_DIR}train.csv') \n",
    "add_embeddings(f'{DATA_DIR}test.csv') \n",
    "add_embeddings(f'{DATA_DIR}val.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Print some information about the selenoprotein content of each dataset.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrain.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlabel \n\u001b[1;32m      3\u001b[0m test_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtest.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlabel \n\u001b[1;32m      4\u001b[0m val_labels \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mval.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, usecols\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mlabel \n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Print some information about the selenoprotein content of each dataset.\n",
    "train_labels = pd.read_csv(f'{DATA_DIR}train.csv', usecols=['label']).label \n",
    "test_labels = pd.read_csv(f'{DATA_DIR}test.csv', usecols=['label']).label \n",
    "val_labels = pd.read_csv(f'{DATA_DIR}val.csv', usecols=['label']).label \n",
    "\n",
    "print('Selenoprotein content of the training dataset:', np.sum(train_labels) / len(train_labels))\n",
    "print('Selenoprotein content of the testing dataset:', np.sum(test_labels) / len(test_labels))\n",
    "print('Selenoprotein content of the validation dataset:', np.sum(val_labels) / len(val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
