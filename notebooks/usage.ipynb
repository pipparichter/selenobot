{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d977b3",
   "metadata": {},
   "source": [
    "# selenobot-detect\n",
    "\n",
    "Before running any of the below code, follow the instructions in the `README` for installing `selenobot` and downloading necessary training, testing, and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71333b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b3bb7f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /home/prichter/Documents/selenobot-detect\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: selenobot\n",
      "  Building wheel for selenobot (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for selenobot: filename=selenobot-0.1-py3-none-any.whl size=28084 sha256=f1399ade498beb4bdbd4d7da20b0b8b290059308cc6e2a1c0eb7f03338c4ce9a\n",
      "  Stored in directory: /home/prichter/.cache/pip/wheels/a0/aa/d8/1eccb2e865f567bdbccad42f9e1546f0e184aab89b2b2568c4\n",
      "Successfully built selenobot\n",
      "Installing collected packages: selenobot\n",
      "  Attempting uninstall: selenobot\n",
      "    Found existing installation: selenobot 0.1\n",
      "    Uninstalling selenobot-0.1:\n",
      "      Successfully uninstalled selenobot-0.1\n",
      "Successfully installed selenobot-0.1\n"
     ]
    }
   ],
   "source": [
    "! pip install /home/prichter/Documents/selenobot-detect/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16548d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b68634",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "`selenobot` uses a modular workflow to train `Classifier` objects on embedding data: **data embedding**, **dataset instantiation**, and **model training**. Each step in the workflow is handled by a separate object, which are described below. \n",
    "\n",
    "1. `Embedder`: The `Embedder` class handles the conversion of amino acid sequences, read in from a FASTA file, to numerical representation. The two defined `Embedders` are a `LengthEmbedder`, which reduces a sequence to its length, and the `AacEmbedder`, which produces a representation of the sequence based on its amino acid composition. \n",
    "2. `Dataset`: The `Dataset` class acts as a storage container for the data, and manages how it can be accessed by the `Classifier` during training and testing.\n",
    "4. `Classifier`: This class defines a binary linear classifier which can be trained to distinguish between full-length proteins and truncated selenoproteins. The output of a trained `Classifier` is a prediction of the identity of the input sequence(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50bbb67",
   "metadata": {},
   "source": [
    "### Data embedding\n",
    "\n",
    "First, we need to instantiate our `Embedder` objects, which will tell the `Dataset` how to manage the data it contains. Because the protein-language model (PLM) embeddings have been pre-computed, we don't need to specify an embedder. The `plm_embedder` variable is set to `None` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57d7535",
   "metadata": {},
   "outputs": [],
   "source": [
    "aac_embedder = selenobot.create_embedder('aac')\n",
    "len_embedder = selenobot.create_embedder('len')\n",
    "# The PLM embeddings have been pre-generated, and are stored in a CSV file.\n",
    "# No additional embedder is needed, but None is used as a placeholder for consistency.\n",
    "plm_embedder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fac4e3",
   "metadata": {},
   "source": [
    "### Dataset instantiation\n",
    "\n",
    "Now that we have our `Embedder` objects, we can load the raw sequence data (and the pre-computed PLM embeddings) into `Dataset` objects. The `Dataset` object uses the input `Embedder` to process the data it loads in from the file at the specified path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "006416c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is stored at: /home/prichter/data/detect/train.csv\n",
      "Validation data is stored at: /home/prichter/data/detect/val.csv\n"
     ]
    }
   ],
   "source": [
    "# Read the paths to the training and validation data from the selenobot.cfg file.\n",
    "# These paths were set during the setup procedure. \n",
    "train_path = selenobot.get_train_data_path()\n",
    "val_path = selenobot.get_val_data_path()\n",
    "\n",
    "print('Training data is stored at:', train_path)\n",
    "print('Validation data is stored at:', val_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b4acf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets for both the training and validation data. \n",
    "aac_dataset, aac_val_dataset = selenobot.create_dataset(aac_embedder, train_path, nrows=10000), selenobot.create_dataset(aac_embedder, val_path, nrows=500)\n",
    "len_dataset, len_val_dataset = selenobot.create_dataset(len_embedder, train_path, nrows=10000), selenobot.create_dataset(len_embedder, val_path, nrows=500)\n",
    "plm_dataset, plm_val_dataset = selenobot.create_dataset(plm_embedder, train_path, nrows=10000), selenobot.create_dataset(plm_embedder, val_path, nrows=500)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1da64ee",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Once we have instantiated the `Datasets` for training and validation, we can begin training!\n",
    "\n",
    "First, we need to create the appropriate `Classifier` for each `Dataset`. The `create_classifier` function detects the embedding type contained in the input `Dataset`, and uses it to choose the appropriate layer dimensions for the `Classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf858195",
   "metadata": {},
   "outputs": [],
   "source": [
    "aac_classifier = selenobot.create_classifier(aac_dataset)\n",
    "len_classifier = selenobot.create_classifier(len_dataset)\n",
    "plm_classifier = selenobot.create_classifier(plm_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c03b2b4",
   "metadata": {},
   "source": [
    "We can now train each model by calling the `train` function. This function uses the input datasets to instantiate a `pytorch` `DataLoader`, which handles the batching of the data for training. The descriptions of the tunable training parameters are given below. \n",
    "\n",
    "    \n",
    "- `model`: A `Classifier` to train on the input Datasets. \n",
    "- `dataset`: A `Dataset` containing the training data. \n",
    "- `val_dataset` A `Dataset` containing the validation data. \n",
    "- `epochs`: The number of epochs to train the model for. \n",
    "- `batch_size`: The size of the batches which the training data will be split into. \n",
    "- `balance_batches`: Whether or not to ensure that each batch has equal proportion of full-length and truncated proteins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11eea6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'epochs':10, 'batch_size':128, 'balance_batches':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eb328b",
   "metadata": {},
   "outputs": [],
   "source": [
    "aac_train_reporter = selenobot.train(aac_classifier, aac_dataset, val_dataset=aac_val_dataset, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57573d32",
   "metadata": {},
   "source": [
    "## Loading existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b7d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
