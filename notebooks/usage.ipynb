{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75d977b3",
   "metadata": {},
   "source": [
    "# selenobot-detect\n",
    "\n",
    "Before running any of the below code, follow the instructions in the `README` for installing `selenobot` and downloading necessary training, testing, and validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40aef78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import selenobot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b68634",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "`selenobot` uses a modular workflow to train `Classifier` objects on embedding data: **data embedding**, **dataset instantiation**, and **model training**. Each step in the workflow is handled by a separate object, which are described below. \n",
    "\n",
    "1. `Embedder`: The `Embedder` class handles the conversion of amino acid sequences, read in from a FASTA file, to numerical representation. The two defined `Embedders` are a `LengthEmbedder`, which reduces a sequence to its length, and the `AacEmbedder`, which produces a representation of the sequence based on its amino acid composition. \n",
    "2. `Dataset`: The `Dataset` class acts as a storage container for the data, and manages how it can be accessed by the `Classifier` during training and testing.\n",
    "4. `Classifier`: This class defines a binary linear classifier which can be trained to distinguish between full-length proteins and truncated selenoproteins. The output of a trained `Classifier` is a prediction of the identity of the input sequence(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119dc126",
   "metadata": {},
   "source": [
    "### Data embedding\n",
    "\n",
    "First, we need to instantiate our `Embedder` objects, which will tell the `Dataset` how to manage the data it contains. Because the protein-language model (PLM) embeddings have been pre-computed, we don't need to specify an embedder. The `plm_embedder` variable is set to `None` for consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8420c743",
   "metadata": {},
   "outputs": [],
   "source": [
    "aac_embedder = selenobot.get_embedder('aac')\n",
    "len_embedder = selenobot.get_embedder('len')\n",
    "# The PLM embeddings have been pre-generated, and are stored in a CSV file.\n",
    "# No additional embedder is needed, but None is used as a placeholder for consistency.\n",
    "plm_embedder = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61352a1",
   "metadata": {},
   "source": [
    "### Dataset instantiation\n",
    "\n",
    "Now that we have our `Embedder` objects, we can load the raw sequence data (and the pre-computed PLM embeddings) into `Dataset` objects. The `Dataset` object uses the input `Embedder` to process the data it loads in from the file at the specified path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3196aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the paths to the training and validation data from the selenobot.cfg file.\n",
    "# These paths were set during the setup procedure. \n",
    "train_path = selenobot.get_train_path()\n",
    "val_path = selenobot.get_val_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98af5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Datasets for both the training and validation data. \n",
    "aac_dataset, aac_val_dataset = selenobot.create_dataset(aac_embedder, train_path), selenobot.create_dataset(aac_embedder, val_path)\n",
    "len_dataset, len_val_dataset = selenobot.create_dataset(len_embedder, train_path), selenobot.create_dataset(len_embedder, val_path)\n",
    "plm_dataset, plm_val_dataset = selenobot.create_dataset(plm_embedder, train_path), selenobot.create_dataset(plm_embedder, val_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edf199e",
   "metadata": {},
   "source": [
    "### Model training\n",
    "\n",
    "Once we have instantiated the `Datasets` for training and validation, we can begin training!\n",
    "\n",
    "First, we need to create the appropriate `Classifier` for each `Dataset`. The "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24dcf6f",
   "metadata": {},
   "source": [
    "## Loading existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf802f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "selenobot",
   "language": "python",
   "name": "selenobot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
