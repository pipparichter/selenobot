{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486daaa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fe41a",
   "metadata": {},
   "source": [
    "# Selenobot usage guide\n",
    "\n",
    "The new Selenobot usage popeline has two main components: `Datasets` and `Classifiers`. These two components are each represented by a base class, from which more specialized versions inherit their core functionality. \n",
    "\n",
    "## Datasets\n",
    "\n",
    "The classes in this category all inherit from the `Dataset` class, which in turn inherits from `torch.utils.data.Dataset` (this means they can be fed into a `torch.utils.data.DataLoader` for easy batching and training). \n",
    "\n",
    "1. `Dataset` is the base class. It provides the `__getitem__` method, as well as some other basic functions.\n",
    "2. `EmbeddingDataset` supports `Datasets` which are made up of embedded sequence data. All objects which inherit from this class also have an `embed` method, which embeds the input sequence data. \n",
    "3. `EsmEmbeddingDataset` inherits from `EmbeddingDataset`. It supports the embedding of amino acid sequences using a pre-trained ESM model. \n",
    "4. `AacEmbeddingDataset` also inherits from `EmbeddingDataset`. It supports the embedding of amino acid sequences in Amino Acid Content (AAC) embedding space. \n",
    "\n",
    "## Classifiers\n",
    "\n",
    "1. `Classifier` is the base class. It inherits from `torch.nn.Module`, and provides `train` and `test` methods. These methods both make use of a `forward` method, which is overwritten in child classes. \n",
    "2. `NextTokenClassifier` is still very much underdevelopment. This will use the ProtGPT2 model to classify non-embedded sequences using next-token prediction (although the exact technique for this is TBD). \n",
    "3. `EmbeddingClassifier` currently consists of a single linear layer with sigmoid activation (it definitely can be improved upon, with dropout layers, etc.) It trains this classifier on pre-generated embeddings to predict whether or not a sequence is a selenoprotein. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa9a622e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.classifiers import *\n",
    "from src.datasets import *\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb710db1",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "### Comparing `EmbeddingClassifiers` using Prot5 and ESM embeddings\n",
    "\n",
    "Because the most immediate question is whether or not the Prot5 embeddings provide results which are as good as the ESM embeddings, I will try this first! I should note that, because I have the ESM embeddings already generated, I don't need to use the `EsmEmbeddingDataset`. This is really just useful for generating the embeddings, or if you wanted to clarify the type of embeddings contained in the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7bab60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/prichter/Documents/selenobot/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd64374",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr5_train_dataset = EmbeddingDataset.from_csv(data_dir + 'train_embeddings_pr5.csv')\n",
    "pr5_test_dataset = EmbeddingDataset.from_csv(data_dir + 'test_embeddings_pr5.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28be0dc",
   "metadata": {},
   "source": [
    "We can extract the latent dimension of the embedding space using the following code. This will then be used as input to the `EmbeddingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8dfe44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = pr5_test_dataset.latent_dim\n",
    "print('Latent dimension of the Prot5 embedding space is', latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d808bc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = EmbeddingClassifier(latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341d9c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr5_train_dataloader = torch.utils.data.DataLoader(pr5_train_dataset, batch_size=64, shuffle=True)\n",
    "pr5_test_dataloader = torch.utils.data.DataLoader(pr5_test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ea7a4eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 18/300 [00:17<04:57,  1.06s/it]"
     ]
    }
   ],
   "source": [
    "losses = classifier.train_(pr5_train_dataloader, test_dataloader=pr5_test_dataloader, epochs=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bee4175",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313f482f",
   "metadata": {},
   "source": [
    "Now that we have a trained model, we can determine what the classification accuracy is. Eventually, I will make a method (or some other thing) to make calculating classification accuracy much easier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257b32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, _ = classifier(torch.Tensor(pr5_test_dataset.data))\n",
    "\n",
    "logits = np.ravel(np.round(logits.detach().numpy())) # Convert to a numpy array for accuracy analysis. \n",
    "labels = pr5_test_dataset.metadata['label']\n",
    "\n",
    "print(logits)\n",
    "print(labels)\n",
    "\n",
    "print('Classification accuracy is', (logits == labels).astype(np.float).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3109407",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
